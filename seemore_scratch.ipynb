{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn  # why\n",
    "from torch.nn import functional as F # why\n",
    "from torch.nn import init  # why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self , img_size=96 , patch_size=16 , hidden_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.num_patches = (img_size // patch_size)**2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=hidden_dim,kernel_size=patch_size,stride=patch_size)\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        X = self.conv(X)\n",
    "\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        X = X.transpose(1,2)\n",
    "\n",
    "        return X\n",
    "\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"./input.txt\"\n",
    "\n",
    "with open(text_path , 'r' , encoding='utf-8') as f :\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "stoi[''] = 65\n",
    "\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "itos[65] = ''\n",
    "# what does lambda function do ?\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "vocab_size = len(stoi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size , patch_size , num_hiddens , batch_size = 96,16,512,4\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(img_size,patch_size,num_hiddens)\n",
    "\n",
    "X = torch.zeros(batch_size , 3, img_size , img_size)\n",
    "\n",
    "patch_embeddings(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets define a MLP class now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self , n_embd , dropout=0.1 , is_decoder=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [\n",
    "\n",
    "                nn.Linear(n_embd , 4 * n_embd),\n",
    "\n",
    "                nn.ReLU() if is_decoder else nn.GELU(),\n",
    "\n",
    "                nn.Linear(4 * n_embd , n_embd),\n",
    "\n",
    "                nn.Dropout(dropout)\n",
    "                ]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets check what will happen if we give it a input embedding of size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 128\n",
    "testmlp = MLP(n_embd)\n",
    "mlp_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testmlp_out = testmlp(mlp_input)\n",
    "testmlp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self , n_embd , head_size , dropout = 0.1 , is_decoder = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embd , head_size , bias=False)\n",
    "\n",
    "        self.query = nn.Linear(n_embd,head_size,bias=False)\n",
    "\n",
    "        self.value = nn.Linear(n_embd,head_size,bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    \n",
    "    def forward(self , x):\n",
    "\n",
    "        B , T , C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * ( C ** -0.5)\n",
    "\n",
    "        if self.is_decoder:\n",
    "\n",
    "            tril = torch.tril(torch.ones(T,T, dtype = torch.bool , device = x.device))\n",
    "\n",
    "            wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "\n",
    "        wei = F.softmax(wei , dim=-1)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd, head_size, batch_size = 128, 16, 4\n",
    "\n",
    "testhead = Head(n_embd, head_size)\n",
    "head_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testhead_out = testhead(head_input)\n",
    "testhead_out.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self , n_embd , num_heads , dropout=0.1 , is_decoder = False):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            assert n_embd % num_heads == 0 , \"n_embd must be divisible by num_heads\"\n",
    "\n",
    "            self.heads = nn.ModuleList([\n",
    "\n",
    "                Head(n_embd , n_embd // num_heads , dropout , is_decoder)\n",
    "                for _ in range(num_heads)\n",
    "\n",
    "\n",
    "            ])\n",
    "\n",
    "            self.proj = nn.Linear(n_embd , n_embd)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self , x):\n",
    "\n",
    "        head_outputs = [h(x) for h in self.heads]\n",
    "\n",
    "        out = torch.cat(head_outputs , dim = -1)\n",
    "\n",
    "        out = self.proj(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd, n_head = 128, 8\n",
    "testmha = MultiHeadAttention(n_embd, n_head)\n",
    "head_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testmha_out = testmha(head_input)\n",
    "testmha_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets code the encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd , num_heads , dropout = 0.1 , is_decoder = False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_embd , num_heads , dropout , is_decoder)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "\n",
    "                nn.Linear(n_embd , 4 * n_embd),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4 * n_embd , n_embd) ,\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self , x):\n",
    "        original_x = x\n",
    "\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        attn_output = self.attn(x)\n",
    "\n",
    "        x = original_x + attn_output\n",
    "\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd, head_size, batch_size = 128, 16, 4\n",
    "\n",
    "testblock = Block(n_embd, n_head)\n",
    "block_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testblock_out = testblock(block_input)\n",
    "testblock_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use all the chunks together to create a ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\n",
    "    def __init__(self , img_size , patch_size , num_hiddens,num_heads , num_blks , emb_dropout, blk_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbeddings(img_size, patch_size , num_hiddens)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,num_hiddens))\n",
    "\n",
    "        num_patches = (img_size // patch_size)**2\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,num_patches + 1 , num_hiddens)) # what does nn.Parameter do??\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(num_hiddens, num_heads, blk_dropout, is_decoder=False) for _ in range(num_blks)])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(num_hiddens)\n",
    "\n",
    "    \n",
    "    def forward(self , X):\n",
    "\n",
    "        x = self.patch_embedding(X)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1 , -1)\n",
    "\n",
    "        x = torch.cat((cls_tokens,x) , dim = 1)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blocks in self.blocks:\n",
    "\n",
    "            x = blocks(x)\n",
    "\n",
    "        x = self.layer_norm(x[:,0])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size, patch_size, num_hiddens, n_head, num_blks, dropout = 96, 16, 512, 8, 3, 0.1\n",
    "\n",
    "testvit = ViT(img_size, patch_size, num_hiddens, n_head, num_blks, dropout, dropout)\n",
    "vit_input = torch.zeros(batch_size, 3, img_size, img_size)\n",
    "testvit_out = testvit(vit_input)\n",
    "testvit_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have the embedding from the images and we need to concatenate with the text embedding , but we can't do that directly . first we need to project the dimensionality of image embeddings to dimensionality of text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    def __init__(self, n_embd, image_embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(image_embed_dim, 4 * image_embed_dim),\n",
    "\n",
    "            \n",
    "            nn.GELU(),\n",
    "\n",
    "            \n",
    "            nn.Linear(4 * image_embed_dim, n_embd),\n",
    "\n",
    "            \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd,num_hiddens = 128, 512\n",
    "\n",
    "testmmp = MultiModalProjector(n_embd,num_hiddens)\n",
    "mmp_input = testvit_out\n",
    "testmmp_out = testmmp(mmp_input)\n",
    "testmmp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, image_embed_dim, vocab_size, num_heads, n_layer, use_images=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_images = use_images\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(1000, n_embd)\n",
    "\n",
    "        if use_images:\n",
    "            # Image projection layer to align image embeddings with text embeddings\n",
    "            self.image_projection = MultiModalProjector(n_embd, image_embed_dim)\n",
    "\n",
    "        # Stack of transformer decoder blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads, is_decoder=True) for _ in range(n_layer)])\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, image_embeds=None, targets=None):\n",
    "        # Get token embeddings from the input indices\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images and image_embeds is not None:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(image_embeds).unsqueeze(1)\n",
    "            tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
    "\n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(tok_emb.size(1), device=device)).unsqueeze(0)\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through the transformer decoder blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Get the logits from the language modeling head\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            if self.use_images and image_embeds is not None:\n",
    "                # Prepare targets by concatenating a dummy target for the image embedding\n",
    "                batch_size = idx.size(0)\n",
    "                targets = torch.cat([torch.full((batch_size, 1), -100, dtype=torch.long, device=device), targets], dim=1)\n",
    "\n",
    "            # Compute the cross-entropy loss\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx, image_embeds, max_new_tokens):\n",
    "        # Get the batch size and sequence length\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Initialize the generated sequence with the input indices\n",
    "        generated = idx\n",
    "\n",
    "        if self.use_images and image_embeds is not None:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(image_embeds).unsqueeze(1)\n",
    "            current_output = torch.cat([img_emb, self.token_embedding_table(idx)], dim=1)\n",
    "        else:\n",
    "            current_output = self.token_embedding_table(idx)\n",
    "\n",
    "        # Generate new tokens iteratively\n",
    "        for i in range(max_new_tokens):\n",
    "            # Get the current sequence length\n",
    "            T_current = current_output.size(1)\n",
    "\n",
    "            # Get position embeddings for the current sequence length\n",
    "            current_pos_emb = self.position_embedding_table(torch.arange(T_current, device=device)).unsqueeze(0)\n",
    "\n",
    "            \n",
    "            current_output += current_pos_emb\n",
    "\n",
    "            \n",
    "            for block in self.blocks:\n",
    "                current_output = block(current_output)\n",
    "\n",
    "            \n",
    "            logits = self.lm_head(current_output[:, -1, :])\n",
    "\n",
    "           \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            \n",
    "            generated = torch.cat((generated, idx_next), dim=1)\n",
    "\n",
    "            \n",
    "            idx_next_emb = self.token_embedding_table(idx_next)\n",
    "\n",
    "            \n",
    "            current_output = torch.cat((current_output, idx_next_emb), dim=1)\n",
    "\n",
    "        return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([10, 51, 1000]), Loss: 7.0855607986450195\n",
      "Generated sequence shape: torch.Size([10, 70])\n"
     ]
    }
   ],
   "source": [
    "model = DecoderLanguageModel(n_embd= 128 , image_embed_dim= 256 , vocab_size= 1000 , num_heads=8 , n_layer= 6 , use_images= True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "B, T = 10 , 50\n",
    "\n",
    "idx = torch.randint(0,1000, (B,T)).to(device)\n",
    "\n",
    "image_embeds = torch.randn(B, 256).to(device)\n",
    "\n",
    "targets = torch.randint(0, vocab_size, (B, T)).to(device) \n",
    "\n",
    "if targets is not None:\n",
    "    logits, loss = model(idx, image_embeds, targets)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
    "else:\n",
    "    logits = model(idx, image_embeds)  # Call without targets\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
    "print(f\"Generated sequence shape: {generated.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets put all the blocks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, num_heads, num_blks, emb_dropout, blk_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set num_hiddens equal to image_embed_dim\n",
    "        num_hiddens = image_embed_dim\n",
    "\n",
    "        # Assert that num_hiddens is divisible by num_heads\n",
    "        assert num_hiddens % num_heads == 0, \"num_hiddens must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize the vision encoder (ViT)\n",
    "        self.vision_encoder = ViT(img_size, patch_size, num_hiddens, num_heads, num_blks, emb_dropout, blk_dropout)\n",
    "\n",
    "        # Initialize the language model decoder (DecoderLanguageModel)\n",
    "        self.decoder = DecoderLanguageModel(n_embd, image_embed_dim, vocab_size, num_heads, n_layer, use_images=True)\n",
    "\n",
    "    def forward(self, img_array, idx, targets=None):\n",
    "        # Get the image embeddings from the vision encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if the image embeddings are valid\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\")\n",
    "\n",
    "        if targets is not None:\n",
    "            # If targets are provided, compute the logits and loss\n",
    "            logits, loss = self.decoder(idx, image_embeds, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            # If targets are not provided, compute only the logits\n",
    "            logits = self.decoder(idx, image_embeds)\n",
    "            return logits\n",
    "\n",
    "    def generate(self, img_array, idx, max_new_tokens):\n",
    "        # Get the image embeddings from the vision encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if the image embeddings are valid\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\")\n",
    "\n",
    "        # Generate new tokens using the language model decoder\n",
    "        generated_tokens = self.decoder.generate(idx, image_embeds, max_new_tokens)\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from initialization forward pass: tensor([[[-0.5262, -0.0951, -0.9254,  ..., -0.1929, -0.3010, -0.1736],\n",
      "         [-0.3884, -0.2352, -0.2910,  ...,  0.1570,  0.2544,  0.7743],\n",
      "         [-0.0957, -0.5236,  0.1347,  ...,  0.4445,  0.2568,  0.4157],\n",
      "         ...,\n",
      "         [ 0.3338,  0.4779, -0.5849,  ..., -0.4378, -0.2896,  0.1432],\n",
      "         [-0.3068,  0.6018,  0.8893,  ...,  0.3438, -0.6156, -0.8650],\n",
      "         [-0.5292,  0.1760, -0.1767,  ..., -0.8456, -0.0648,  0.2146]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_embed_dim = num_hiddens\n",
    "\n",
    "n_layer, block_size =  8, 32\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(n_embd, image_embed_dim, vocab_size,  n_layer, img_size, patch_size, n_head, num_blks, dropout, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Create dummy data with correct dimensions\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)  # Correct shape for image input\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)  # Correct shape for text input\n",
    "\n",
    "# Forward pass to initialize all parameters\n",
    "try:\n",
    "    output = model(dummy_img, dummy_idx)  # Output for debugging\n",
    "    print(\"Output from initialization forward pass:\", output)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
    "    print(\"Check layer configurations and input shapes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def base64_to_tensor(base64_str, img_size=96):\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(base64_str)))\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df, batch_size, split='train', img_size=96, val_batch_size=8):\n",
    "    # Split data into training and validation sets\n",
    "    n = int(0.9 * len(df))  # first 90% will be train, rest val\n",
    "    df_train = df.iloc[:n]\n",
    "    df_val = df.iloc[n:]\n",
    "    data = df_train if split == 'train' else df_val\n",
    "    batch_size = batch_size if split == 'train' else val_batch_size\n",
    "    replace = False if split == 'train' else True\n",
    "    batch = data.sample(n=batch_size, replace=replace)\n",
    "\n",
    "    images = torch.cat([base64_to_tensor(img, img_size) for img in batch['b64string_images']], dim=0).to(device)\n",
    "    text_indices = [torch.tensor(encode(desc), dtype=torch.long) for desc in batch['caption']]\n",
    "    max_length = max(len(t) for t in text_indices)\n",
    "\n",
    "    padded_text = torch.full((batch_size, max_length), fill_value=stoi[''], dtype=torch.long).to(device)\n",
    "    for i, text in enumerate(text_indices):\n",
    "        padded_text[i, :len(text)] = text\n",
    "\n",
    "    targets = torch.cat([padded_text[:, 1:], torch.full((batch_size, 1), fill_value=stoi[''], dtype=torch.long, device=device)], dim=1)\n",
    "\n",
    "    # Truncate or pad targets to match the length of padded_text\n",
    "    if targets.size(1) > padded_text.size(1):\n",
    "        targets = targets[:, :padded_text.size(1)]\n",
    "    elif targets.size(1) < padded_text.size(1):\n",
    "        targets = torch.cat([targets, torch.full((batch_size, padded_text.size(1) - targets.size(1)), fill_value=stoi[''], dtype=torch.long, device=device)], dim=1)\n",
    "\n",
    "    return images, padded_text, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, df, epochs, vocab_size, img_size=96):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for _ in range(max_iters):\n",
    "            images, idx, targets = get_batch(df, batch_size, 'train', img_size)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(images, idx, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if _ % eval_interval == 0:\n",
    "                print(f\"Loss at iteration {_}: {loss.item()}\")\n",
    "        val_loss = estimate_loss(model, df, 'val', img_size, val_batch_size=8)\n",
    "        print(f\"Validation Loss after epoch {epoch}: {val_loss}\")\n",
    "\n",
    "def estimate_loss(model, df, split, img_size=96, val_batch_size=8):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for _ in range(eval_iters):\n",
    "        images, idx, targets = get_batch(df, batch_size, split, img_size, val_batch_size=val_batch_size)\n",
    "        _, loss = model(images, idx, targets)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 4.3069024085998535\n",
      "Loss at iteration 10: 0.1717265248298645\n",
      "Loss at iteration 20: 0.08332188427448273\n",
      "Loss at iteration 30: 0.06375972926616669\n",
      "Loss at iteration 40: 0.06932108849287033\n",
      "Loss at iteration 50: 0.08294254541397095\n",
      "Loss at iteration 60: 0.07798963040113449\n",
      "Loss at iteration 70: 0.05572696775197983\n",
      "Loss at iteration 80: 0.03254299238324165\n",
      "Loss at iteration 90: 0.0669950470328331\n",
      "Validation Loss after epoch 0: 0.03557506361976266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume existing stoi, itos, and vocab_size are already defined\n",
    "\n",
    "# Modify the encode function to handle unknown characters\n",
    "def encode(s):\n",
    "    return [stoi.get(c, stoi['']) for c in s]  # Use '' for unknown characters\n",
    "\n",
    "# Update get_batch function\n",
    "def get_batch(df, batch_size, split='train', img_size=96, val_batch_size=None):\n",
    "    # Split data into training and validation sets\n",
    "    n = int(0.9 * len(df))  # first 90% will be train, rest val\n",
    "    df_train = df.iloc[:n]\n",
    "    df_val = df.iloc[n:]\n",
    "    \n",
    "    data = df_train if split == 'train' else df_val\n",
    "    current_batch_size = batch_size if split == 'train' else (val_batch_size or batch_size)\n",
    "    replace = False if split == 'train' else True\n",
    "    \n",
    "    batch = data.sample(n=current_batch_size, replace=replace)\n",
    "    \n",
    "    images = torch.cat([base64_to_tensor(img, img_size) for img in batch['b64string_images']], dim=0).to(device)\n",
    "    text_indices = [torch.tensor(encode(desc), dtype=torch.long) for desc in batch['caption']]\n",
    "    max_length = max(len(t) for t in text_indices)\n",
    "    \n",
    "    padded_text = torch.full((current_batch_size, max_length), fill_value=stoi[''], dtype=torch.long).to(device)\n",
    "    for i, text in enumerate(text_indices):\n",
    "        padded_text[i, :len(text)] = text\n",
    "    \n",
    "    targets = torch.cat([padded_text[:, 1:], torch.full((current_batch_size, 1), fill_value=stoi[''], dtype=torch.long, device=device)], dim=1)\n",
    "    \n",
    "    # Truncate or pad targets to match the length of padded_text\n",
    "    if targets.size(1) > padded_text.size(1):\n",
    "        targets = targets[:, :padded_text.size(1)]\n",
    "    elif targets.size(1) < padded_text.size(1):\n",
    "        targets = torch.cat([targets, torch.full((current_batch_size, padded_text.size(1) - targets.size(1)), fill_value=stoi[''], dtype=torch.long, device=device)], dim=1)\n",
    "    \n",
    "    return images, padded_text, targets\n",
    "\n",
    "# The rest of your code remains the same\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 100\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "epochs = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 40\n",
    "num_blks = 3\n",
    "head_size = 16\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "image_embed_dim = 512\n",
    "emb_dropout = blk_dropout = 0.1\n",
    "\n",
    "# Assuming you have defined VisionLanguageModel and train_model functions\n",
    "model = VisionLanguageModel(n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, n_head, num_blks, emb_dropout, blk_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Dummy data to initialize lazy modules\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)\n",
    "model(dummy_img, dummy_idx)  # Forward pass to initialize all parameters\n",
    "\n",
    "# Train the model\n",
    "train_model(model, df, epochs, vocab_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI702",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
